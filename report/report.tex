\documentclass{article}
\usepackage[final]{nips_2018}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{hyperref}
\usepackage{url}
\usepackage{booktabs}
\usepackage{amsfonts}
\usepackage{nicefrac}
\usepackage{microtype}

\title{Learning to Grasp: From the Cornell Dataset}
%\author{George Maratos (need to mention advisor)}

\begin{document}
\maketitle

\begin{abstract}
Fully autonomous grasping is a difficult problem. In this project, I explored
the cornell grasping dataset and tried to see if I could build a model that
learning how to model the information from this. The task to be learned was
difficult, and the dataset was small, so I tried to mitigate these challenges
with pretraining.
\end{abstract}

\section{Introduction}
In the robotic grasping problem the goal is, given an object, select a grasp
configuration such that the object can be restrained and
manipulated to some desirable end. Finding such configurations is difficult
because of the multi-modal nature of the input and the fact that there can be
more than one suitable grasping location, leaving machines with the task
of determing optimality for predicted configurations.

Some of the earliest reviews of algorithms for grasping \cite{shimoga96,bicchi00},
shows that the premliminary work involved solving unconstrained linear programming
problems using objectives that measure dexterity and grasp quality. A grasp
algorithm in this context is one that is able to calculate the stability or
equilibrium of its grasp, and they are collectively refered too in the
review as \textit{robot grasp synthesis algorithms}.

The review by Sahbani \textit{et al.} \cite{sahbani12}, makes a distinction
between methods that model the kinematics and dynamics of a grasp, like the
synthesis algorithms, and methods that mimic human grasp strategies or learn
from data. The former called the analytic methods and the latter empirical.
They divide the analytical techniques into force closure methods and task
oriented. The authors determine that force closure is able to find stable
grasps but are usually not task oriented, and the task oriented strategies tend
to be computationally intensive. The empirical methods on the other hand can
model task oriented features, but struggle to generalize well to new objects.

Bohg \textit{et al.} \cite{bohg14} observe that grasping methods typically aim
to address the following object types: fully known, familiar, or fully unknown.
When considering the empirical methods, fully known represents objects that
have been seen in the training data before and the grasping problem reduces to
locating the object and applying a similar grasp to those from experience.
The familiar objects are assumed to have some matching characteristics to objects
from the training data, like shape, color, and texture. Familiar objects
introduce a new challenge of calculating similarity between objects so that
the appropriate grasp can be determined. On the other hand, grasping algorithms
will have no experience to utilize when approaching fully known objects. Methods
of this category typically rely on finding structures in the features to
synthesize a grasp.

The focus of this work is to build empirical models for grasp synthesis using
the Cornell Grasp Datset \cite{lenz15}, and evaluating them on familiar and
fully unknown objects. Section 2 will describe previous methods for grasp
synthesis. Section 3 will focus on analysis of the dataset and a description
of the task. Section 4 will contain the experimental section. Finally, Section
5 is the conclusion and future works.

\section{Existing Grasp Synthesis Methods}
\subsection{Analytical Methods}
The earliest mention of qualities of a successful grasp, to the author's
knowledge, is from \textit{Kinematics of Machinery} by Franz Reuleaux
\cite{reuleaux76}.
In it, constant forces are applied to an object and it is considered
constrained if sensible external forces can be balanced. When the
object is in equilibrium, which occurs if the above conditions are
met, then force closure occurs. Nguyen \textit{et al.} \cite{nguyen86},
explored the notion of force closure and developed algorithms for computing
the set of all force closure grasps on simple shapes.
This work is extended by Ferrari \textit{et al.} \cite{ferrari92}, to calculate
a grasp quality criteria. The criteria is measured as the ratio between the force
applied externally and by the fingers. The best grasp is determined by solving
an optimization problem that minimizes the finger force but still can maintain
force closure against a large external force.

Due to the multimodal nature of the task, it could be useful to model various
features of the object being grasped. In the work by Zhang \textit{et al.}
\cite{zhang12}, the authors explore bayesian filtering in $G-SL(AM)^2$. They
build probabilistic models that model various features: the shape of the,
object, mass, and friction coefficient for example. The application of this
is for when you have input data blackout, which could occur if the arm
during operation occludes the visual sensors.

The simulation software \textit{GraspIt!} \cite{miller04}, was developed with
the goal of aiding research in robotic grasping. It has a wide set of features
such as, many different types of hand models, collision detection, and grasp
quality evaluation. It could be used in setting where expensive robotic hardware
is unavailable or used in an algorithm that evaluates grasps in the simulation
before choosing the best one for a physical robot.
The software was applied in work done my Miller
\textit{et al.} \cite{miller03}, where they designed a grasp planning algorithm
that modeled objects as shape primitives and generated poses based off the
primitives.

\subsection{Empirical Methods}
The empirical methods involve techniques that implement learning algorithms that
model the grasping problem from data. This section will only discuss the
non-deep learning methods, deep learning is discussed in another section.

The simulator Graspit! \cite{miller04} enabled researchers to collect synthetic
data for grasping, one example is by Pelossof \textit{et al.} \cite{pelossof04}.
The authors generated synthetic data by subsampling the parameters from
generated grasps and they trained an
SVM-RBF regressor with the goal of predicting grasp quality. They generated
grasps by fixing certain joints, for example to guarantee the palm runs
parallel with the surface of the object, and allowing others to be sampled
randomly within a range. Using their procedure, the authors were able to
generate a dataset with approximately $80\%$ of the grasps having a non-zero
graspability score. Graspability being calculated in the same way as previous
works \cite{ferrari92}.

Robots that are predicting grasps will not always have a complete model of the
input, for example visual sensors might not be able to view an object from all
directions. Kamon \textit{et al.} \cite{kamon96} attempt to predict
grasps from only 2-d images of the object. They solve two problems
simultaneously: choosing grasping points, and predicting the quality of a grasp.
About 2000 grasping examples were collected, which were used for prediction during
testing. Features are extracted directly from the image, for example center of
mass, and were used to calculate the quality of a grasp. The authors claim
that it can be predicted reliably using only local features.
%finish this

Learning a grasping point in the image plane may be easier for learning algorithms
but they do not model the gripper configuration completely. The work by Jiang
\textit{et al.}
\cite{jiang11}, learn a rectangle configuration that can better represent a parallel
gripper plate end effector. A rectangle, in this case, has $5$ parameters which
correspond to the 2-d coordinate of the top left corner of the rectangle, the height
and width, and the angle between the first edge and the x-axis. They define a score
function, which is a linear combination of features, that represents the quality of
a grasp rectangle. The features are calculated from the pixels within the rectangle, and
consist of 17 filters and 5-bin histograms as a set of quick features and a collection of
more advanced non-linear features are used as well when doing the final inference.
Inference is done in a two step process,
with two separate SVM ranking models. The first model ranks using the features that
are easy to compute, and this prunes the space of possible rectangles. The second ranks
the rectangles chosen by the first model. For calculating the rectangles on an angle,
they rotate the image in discrete intervals of $15\circ$ before calculating the features.

One approach to leveraging empirical data is to apply a non-parametric KNN
model similar to previous works \cite{zhang11,ciocarlie14}.
The objects are represented geometrically and a good grasp is modeled based
on the quality of grasps from previous examples in the same coordinate position.
A simple discriminator is used to compute a binary score for a grasp from the
dataset and a new example. It compares the coordinates from both grasps and
returns $0$ if they contain grasps or do not, and returns $1$ if they do not
match. Using this metric, they model the probability of a good grasp occuring
at a coordinate by considering the ratio of good grasps and bad grasps in the
set of nearest neighbors.

A few works have combined visual features with haptic feedback
\cite{coelho01,piater02}. In their experiments, they probe the objects with
haptic sensors and learn features that would be useful for determining a
stable grasp.

%this needs to be rewritten because I cite other work that uses visual features
Other works \cite{saxena07,saxena08,saxena08a} proposed
algorithms that used visual features to predict grasps, with the distinction from

previous work being that the objects are only partially observable from sets of
2-d images. They extract features that represent edges, textures, and color.
The data was synthetic, constructed using a graphics library called PovRay.
The task is to predict whether a location, which is the center of an image patch,
contains a grasp. The modeling is done using logistic regression, and predicting
the 3-d location is done using a simple probablistic model with naive Bayes
assumptions.

\subsection{Summary of goals}
There are some research papers on the work (early), which are not necessarily
about solving with learning.
The paper associated with
this dataset is also interesting. Hanbo's paper tries a different approach.
There is also pinto and gupta and google which take different approaches
involving learning from attempting grasps. Another approach involves using
techniques from object detection architectures like faster rcnn and ssd.


\section{Data Analysis}
I ran a series of tests, examining the input and targets as I change the task.
I take a look at how I could apply linear regression to the task and see if
I can get any results. I look at the depth data and see how I can extract
features from it. I look at how data augmentation affects the task, and the
clamping as well.

\section{Experiments}
I talk about the hardware I have access too. I trained some networks. I try
to run some ablation experiments with and without features I extracted (and
modification) in data analysis.

\bibliographystyle{plain}
\bibliography{bibliography.bib}
\end{document}
